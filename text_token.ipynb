{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "import pandas as pd\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pd.read_excel('CXIRG_Data\\\\train_data\\\\reports.xlsx', engine='openpyxl')\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = '[CLS] ' + text\n",
    "    text = re.sub('_x000D_', ' ', text)\n",
    "    text = re.sub('[0-9]\\)|>|-|[0-9]\\.|', '', text)\n",
    "    # print(text)\n",
    "    gptpat = re.compile(r\"\"\"\\[[C][L][S]]|\\n|[:.,]| [L]4+| *3+[rd]+| *4+[th]+| [LR]\\'t|[LR]\\'t| T[0-9]+|T[0-9]+| [a-zA-Z]/[a-zA-Z]|[a-zA-Z]/[a-zA-Z]| ?\\p{L}+| ?\\p{N}+\"\"\")\n",
    "    text = re.findall(gptpat, text)\n",
    "    tokens = []\n",
    "    for token in text:\n",
    "        if len(token) > 0:\n",
    "            tokens.append(token)\n",
    "    \n",
    "    def check_token_head(tokens):\n",
    "        while tokens[0] == ' ':\n",
    "            tokens = tokens[1:]\n",
    "            \n",
    "        return tokens\n",
    "    \n",
    "    tokens = check_token_head(tokens)\n",
    "    \n",
    "    # print(len(tokens))\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "report_texts = report['text'].apply(preprocess_text)\n",
    "# report_texts[17]\n",
    "len(report_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dict(df):\n",
    "    vocab_dict = {}\n",
    "    for row in df:\n",
    "        for token in row:\n",
    "            vocab_dict[token] = vocab_dict.get(token, 0) + 1\n",
    "    return vocab_dict\n",
    "\n",
    "vocab_dict = create_dict(report_texts)\n",
    "sorted_dict = {k: v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "len(sorted_dict) ,sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word piece tokenize\n",
    "def build_vocab(word_dict):\n",
    "    itos = {}\n",
    "    stoi = {}\n",
    "    for i, token in enumerate(word_dict.items()):\n",
    "        stoi[token[0]] = i\n",
    "        itos[i] = token[0]\n",
    "    return itos, stoi\n",
    "\n",
    "itos, stoi = build_vocab(sorted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda s : [stoi[c] for c in s]\n",
    "decode = lambda l : ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]\\nChest plain film shows:\\nImpression:\\nCompatible with rightsided aortic arch with aberrant left subclavian artery and Kommerell diverticulum.\\nSuspect bilateral lower lung patches.\\nIncreased infiltrations in both lungs.\\nBlunting bilateral CP angles.\\nTortuous atherosclerotic dilated aorta.\\nCardiomegaly.\\nScoliosis, DJD and osteoporosis of spine.\\n Fracture of right ribs.\\nS/P Rt subclavian CVC insertion.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''\n",
    "Chest plain film shows:\n",
    "Impression:\n",
    "-Compatible with right-sided aortic arch with aberrant left subclavian artery and Kommerell diverticulum.\n",
    "-Suspect bilateral lower lung patches. \n",
    "-Increased infiltrations in both lungs.\n",
    "-Blunting bilateral CP angles.\n",
    "-Tortuous atherosclerotic dilated aorta.\n",
    "-Cardiomegaly.\n",
    "-Scoliosis, DJD and osteoporosis of spine.\n",
    " Fracture of right ribs. \n",
    "S/P Rt subclavian CVC insertion. \n",
    "'''\n",
    "\n",
    "# encode(preprocess_text(text))\n",
    "decode(encode(preprocess_text(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer():\n",
    "    \n",
    "    def __init__(self, word_dict):\n",
    "        self.word_dict = word_dict\n",
    "        \n",
    "        def build_vocab(word_dict):\n",
    "            itos = {}\n",
    "            stoi = {}\n",
    "            for i, token in enumerate(word_dict.items()):\n",
    "                stoi[token[0]] = i\n",
    "                itos[i] = token[0]\n",
    "            return itos, stoi\n",
    "        \n",
    "        self.itos, self.stoi = build_vocab(self.word_dict)\n",
    "        self.encode_ = lambda s : [stoi[c] for c in s]\n",
    "        self.decode_ = lambda l : ''.join([itos[i] for i in l])\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.encode_(text)\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        return self.decode_(tokens)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import math\n",
    "\n",
    "class CustomReportDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, text_df, split='train', word_dict=None):\n",
    "        assert split in ['train', 'valid', 'test']\n",
    "        n_samples = {\n",
    "            'train' : len(text_df) * 0.9,\n",
    "            'valid' : len(text_df) * 0.1,\n",
    "            'test' : len(text_df),\n",
    "        }[split]\n",
    "        if type(text_df) is not list:\n",
    "            text_df = list(text_df)\n",
    "        self.df = random.sample(text_df, int(n_samples))\n",
    "        self.tokenizer = CustomTokenizer(word_dict)\n",
    "        \n",
    "    def __getitem__(self, index) :\n",
    "        target = self.df[index]\n",
    "        target = torch.tensor(self.tokenizer.encode(target))\n",
    "        return target\n",
    "        \n",
    "    # def decode(self, tokens):\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = tokenizer(sorted_dict)\n",
    "train_df = CustomReportDataset(report_texts, split='train', word_dict=sorted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomBlockSeq2Batch(df, block_size, batch_size, threshold=50, device=None, target_idx=None):\n",
    "    \n",
    "    # get rid of the sequence that len < threshold\n",
    "    # n_df = []\n",
    "    # for idx, data in enumerate(df):\n",
    "    #     if len(data) >= threshold: n_df.append(data)\n",
    "    \n",
    "    # get random batch\n",
    "    if target_idx == None: target_idx = random.randint(0, len(df) - 1)\n",
    "    ix = torch.randint(len(df[target_idx]) - block_size, (batch_size, ))\n",
    "    ix[0] = 0                                 # test for make sure CLS\n",
    "    x = torch.stack([df[target_idx][i:i+block_size] for i in ix])\n",
    "    y = torch.stack([df[target_idx][i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = CustomBlockSeq2Batch(train_df, block_size, batch_size, device='cpu')\n",
    "# x, y, x.shape, y.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing batch\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(8):\n",
    "        context = x[b, :t+1]\n",
    "        target = y[b, t]\n",
    "        print(f'when input is {decode(context.tolist())} the target : {(target)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_head, n_embd):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        \n",
    "        self.c_attn = nn.Linear(n_embd, n_embd * 3)\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # batch_size, Seq_len, embedding dim\n",
    "        B, T, C = x.shape\n",
    "        # print(x.shape)\n",
    "        # after c_attn(x), the shape is B, T, n_embd * 3\n",
    "        a = self.c_attn(x)\n",
    "        q, k, v = a.split(self.n_embd, dim=2)\n",
    "        # start view() & transpose()\n",
    "        # shape after transpose (Batch_size, n_head, Seq_len, n_embd // n_head) \n",
    "        # or (B, n_head, T, C // n_head)\n",
    "        q = q.view(B, T, self.n_head, self.n_embd // self.n_head).transpose(2, 1)\n",
    "        k = k.view(B, T, self.n_head, self.n_embd // self.n_head).transpose(2, 1)\n",
    "        v = v.view(B, T, self.n_head, self.n_embd // self.n_head).transpose(2, 1)\n",
    "        # the formula : softmax(QK^T / sqrt(embd_dim(k)))V\n",
    "        # shape after q @ k : (B, n_head, T, T) \n",
    "        attn = q @ k.transpose(-2, -1) * (1 / math.sqrt(self.n_embd * 3 // self.n_head))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        # shape after attn @ v : (B, n_head, T, C // n_head)\n",
    "        y = attn @ v\n",
    "        y = y.transpose(2, 1).contiguous().view(B, T, C)\n",
    "        self.out = self.c_proj(y)\n",
    "        return self.out   \n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_embd, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, n_embd)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape (B, T, C)\n",
    "        x = x + self.sa(self.ln1(x))        # (B, T, C)\n",
    "        x = x + self.ffwd(self.ln2(x))      # (B, T, C)\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, block_size, n_embd, n_head, device, n_layer=8):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = torch.nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = torch.nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        B, T = idx.shape\n",
    "        \n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=self.device)) # (T, C)\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "        \n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            idx_cond = idx[:, -block_size:] # prevent longer block_size, because we just have pos. embd\n",
    "            logits, loss = self(idx_cond) # now (B, T, C)\n",
    "            logits = logits[:, -1, :] # now get the last step and shape (B, C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(stoi)\n",
    "block_size = 8\n",
    "batch_size = 32\n",
    "n_embd = 512\n",
    "n_head = 32\n",
    "lr = 1e-5\n",
    "max_iters = 10000\n",
    "n_layer = 32\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(7.0380, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " tensor([[-0.5126, -1.4193,  1.0005,  ..., -1.5657, -2.2598,  0.0378],\n",
       "         [-0.3944, -0.5534,  2.1017,  ...,  0.5943, -1.9581, -0.7326],\n",
       "         [-1.1588, -0.9734,  0.7828,  ...,  0.1855, -0.3597, -0.7771],\n",
       "         ...,\n",
       "         [-1.6233, -1.5340,  2.1736,  ...,  2.1717, -1.7226, -0.5432],\n",
       "         [ 0.7713, -0.7180,  1.4409,  ...,  0.9081, -0.4820,  0.9586],\n",
       "         [ 0.5734, -0.1359,  0.9323,  ...,  0.9893,  0.3333, -1.1376]],\n",
       "        device='cuda:0', grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Decoder(vocab_size=vocab_size, block_size=block_size, n_embd=n_embd, n_head=n_head, n_layer=n_layer, device='cuda')\n",
    "\n",
    "# test model\n",
    "x, y = CustomBlockSeq2Batch(train_df, block_size, batch_size, device='cuda')\n",
    "m = model.to(device)\n",
    "\n",
    "logits, loss = m(x, y)\n",
    "loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 / 10000 loss : 7.21426\n",
      "------gen------\n",
      "[CLS] Blunted conduction CompatibleCompatible, airspace SuspIncreased ray occupying lateral consolidation internal Cardiomegalys/p Susp lobectomy Pulmonary lateral lateral nodules nodules partialP fields Osteophytes evaluation bony ViewCalcified\n",
      " notIMP out Scoliosis nodule outStent CVP of may infiltrations vein conductionRadiopaque hilaeBlunting nodules pneumoniaRight consider heart R't heartCompatible aortic artery\n",
      "PresenceDilated\n",
      "---------------\n",
      " 1000 / 10000 loss : 0.50224\n",
      "------gen------\n",
      "[CLS] Chest Spondylosis CT may, favor ruled of right middle lung fields.\n",
      "Elevation of right hemidiaphragm.\n",
      " Scoliosis of right hemidiaphragm right hemidiaphragm.\n",
      "DJD of right hemidiaphragm.\n",
      "DJD of right hemidiaphragm.\n",
      "DJD of spine.\n",
      "DJD of spine.\n",
      "S/P right upper abdomen.\n",
      "S/P right upper chest\n",
      "---------------\n",
      " 2000 / 10000 loss : 0.07688\n",
      "------gen------\n",
      "[CLS] Chest AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP AP\n",
      "---------------\n",
      " 3000 / 10000 loss : 0.11417\n",
      "------gen------\n",
      "[CLS] Chest Plain Chest Chest Plain Film:\n",
      " Normal heart size with atherosclerotic aorta.\n",
      " No mediastinal DJD and DJD and osteoporosis and osteoporosis and osteoporosis and osteoporosis and osteoporosis and osteoporosis and osteoporosis and osteoporosis and osteoporosis and osteoporosis and osteoporosis and osteoporosis and osteoporosis and osteoporosis and osteoporosis and osteoporosis and osteoporosis and osteoporosis and osteoporosis and\n",
      "---------------\n",
      " 4000 / 10000 loss : 0.02538\n",
      "------gen------\n",
      "[CLS] Chest metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis metastasis\n",
      "---------------\n",
      " 5000 / 10000 loss : 0.03745\n",
      "------gen------\n",
      "[CLS] Chest Plain Film brachiocephalic Plain Film.\n",
      "Increased infiltrations in bilateral renal fixation in bilateral renal in bilateral renal in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in\n",
      "---------------\n",
      " 6000 / 10000 loss : 0.01730\n",
      "------gen------\n",
      "[CLS] Chest Plain Film No obvious lung mass nor consolidation patch.\n",
      " Normal heart size.\n",
      " No mediastinal widening.\n",
      " stenting widening.\n",
      " Both costophrenic angles are angles are sharp Kommerell diverticulum angles are sharp.\n",
      " Spondylosis of spine.\n",
      " Old fracture of left 3rd and 4th 3rd and 4th and 4th and thickening and\n",
      "---------------\n",
      " 7000 / 10000 loss : 0.02967\n",
      "------gen------\n",
      "[CLS] Chest film shows:\n",
      "Impression:\n",
      "Suspect right upper lung patch.\n",
      "Normal heart size.\n",
      "s/p endotracheal tube insertion.\n",
      " s/p subclavian PortAcath. R/O osteoporosis.\n",
      " R/O ankylosing spondylitis.\n",
      " S/P cervicothoracic spine ankylosing spondylitis.\n",
      " S/P cervicothoracic spine.\n",
      " S/P cervicothoracic spine operation with internal fixations.\n",
      "\n",
      "---------------\n",
      " 8000 / 10000 loss : 0.01377\n",
      "------gen------\n",
      "[CLS] Chest Plain Film:\n",
      "\n",
      "\n",
      "Impression:\n",
      " S/P sternotomy with wire fixation.\n",
      " Surgcial clips in Lt medial lung.\n",
      " Increased both lung markings.\n",
      " S/P atelectasis.\n",
      " S/P Rt jugular Permcath.\n",
      " S/P right pleural pigtail tube.\n",
      " S/P right pleural pigtail tube.\n",
      " S/P right pleural pigtail tube\n",
      "---------------\n",
      " 9000 / 10000 loss : 0.04848\n",
      "------gen------\n",
      "[CLS] Chest Plain Plain Film. Fracture of bilateral CP angles.\n",
      " Elevated right hemidiaphragm.\n",
      "Normal heart size.\n",
      "Spondylosis of spine.\n",
      "S/P NG and endotracheal tube.\n",
      " S/P left chest tube insertion.\n",
      "Subcutaneous emphysema in bilateral neck and left chest walls.\n",
      "Atherosclerotic aorta.\n",
      "Fibrotic lesion at right\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=lr)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    \n",
    "    xb, yb = CustomBlockSeq2Batch(train_df, block_size, batch_size, device='cuda', target_idx=iter%len(train_df))\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if iter % 1000 == 0:\n",
    "        print(f'{iter:5d} / {max_iters} loss : {loss.item():.5f}')\n",
    "        context = torch.tensor([[stoi['[CLS]']]], dtype=torch.long, device=device)\n",
    "        print('------gen------')\n",
    "        print(decode(m.generate(context, max_new_tokens=60)[0].tolist()))\n",
    "        print('---------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Chest Plain Film:\n",
      "\n",
      " Patchy opacified lesion at bilateral lung fields.\n",
      " Obscured bilateral costophrenic angles.\n",
      " Tortuous atherosclerotic dilated aorta.\n",
      " Scoliosis and DJD of spine.\n",
      " Old fracture of right ribs.\n",
      "Atherosclerotic aorta.\n",
      "S/P tracheostomy and endotracheal tube\n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor([[stoi['[CLS]']]], dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=50)[0].tolist()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
